{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Problem 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "raw_df=pd.read_csv('climate_change_1.csv')\n",
    "# print(raw_df)\n",
    "# raw_df.insert(0,'intercept',np.ones(len(raw_df)))\n",
    "X_train=raw_df[raw_df['Year']<=2006].iloc[:,:-1]\n",
    "y_train=raw_df[raw_df['Year']<=2006].iloc[:,-1]\n",
    "X_test=raw_df[raw_df['Year']>2006].iloc[:,:-1]\n",
    "y_test=raw_df[raw_df['Year']>2006].iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.![](E:/formula.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -1.42447519e+02   8.23965026e-03  -3.61230805e-03   6.44651665e-02\n",
      "   2.51139086e-03   1.87173453e-04  -1.63078506e-02  -6.27428705e-03\n",
      "   3.42652461e-03   9.51816177e-02  -1.54295992e+00]\n"
     ]
    }
   ],
   "source": [
    "def closed_form_1(X_train,y_train):\n",
    "    X=X_train.copy()\n",
    "    X.insert(0,'intercept',np.ones(len(X)))\n",
    "    return np.dot(np.matmul(np.linalg.inv(np.matmul(X.T,X)),X.T),y_train)\n",
    "\n",
    "theta=closed_form_1(X_train,y_train)\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_Square of training set: 0.754942294039\n",
      "R_Square of test set: 0.19080180095\n"
     ]
    }
   ],
   "source": [
    "y_hat=theta[0]+np.dot(X_train,theta[1:])\n",
    "y_mean=np.mean(y_train)\n",
    "# print(y_average)\n",
    "print(\"R_Square of training set:\",end=\" \")\n",
    "print(1-np.dot(y_hat-y_train,y_hat-y_train)/np.dot(np.array(y_train)-y_mean,np.array(y_train)-y_mean))\n",
    "# print(np.array(y_train))\n",
    "\n",
    "\n",
    "y_test_hat=theta[0]+np.dot(X_test,theta[1:])\n",
    "y_test_mean=np.mean(y_test)\n",
    "# print(y_average)\n",
    "print(\"R_Square of test set:\",end=\" \")\n",
    "print(1-np.dot(y_test_hat-y_test,y_test_hat-y_test)/np.dot(np.array(y_test)-y_test_mean,np.array(y_test)-y_test_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. np.matmul(X.T,X) should be convertible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5.84465063e-03   1.53379822e-04  -7.87101642e-04  -6.94014402e-04\n",
      "    2.70381958e-05  -4.90718016e-03   3.19428906e-04  -2.56914841e-04\n",
      "   -7.23252880e-03   2.87903073e-02]\n",
      " [  1.53379822e-04   4.08945747e-04  -5.22636051e-05   3.10265502e-04\n",
      "    4.77013490e-07  -8.63891353e-04   2.20094857e-05  -1.37225804e-05\n",
      "   -1.10674856e-04   3.18922973e-03]\n",
      " [ -7.87101642e-04  -5.22636050e-05   4.52040615e-03  -2.16745693e-05\n",
      "    3.18467358e-05   1.21510425e-03   2.81978764e-06  -2.99127059e-05\n",
      "    8.47296501e-04  -5.11070462e-02]\n",
      " [ -6.94014402e-04   3.10265502e-04  -2.16745694e-05   9.17141439e-04\n",
      "    1.07037469e-05  -1.33460950e-03  -1.32613330e-05   8.62639825e-06\n",
      "    1.05924755e-03   5.71017492e-03]\n",
      " [  2.70381959e-05   4.77013489e-07   3.18467358e-05   1.07037469e-05\n",
      "    2.94823828e-05  -1.60623261e-04  -6.57178837e-06  -1.31976652e-05\n",
      "   -3.73774102e-05   7.06930216e-04]\n",
      " [ -4.90718016e-03  -8.63891353e-04   1.21510425e-03  -1.33460950e-03\n",
      "   -1.60623261e-04   1.27222094e-02   6.24134294e-04  -2.47859467e-04\n",
      "    4.79983934e-03  -5.73830693e-02]\n",
      " [  3.19428905e-04   2.20094857e-05   2.81978771e-06  -1.32613330e-05\n",
      "   -6.57178837e-06   6.24134294e-04   2.70881703e-04  -1.54126672e-04\n",
      "   -5.91317235e-04  -4.03001128e-03]\n",
      " [ -2.56914841e-04  -1.37225804e-05  -2.99127059e-05   8.62639825e-06\n",
      "   -1.31976652e-05  -2.47859468e-04  -1.54126672e-04   1.01403289e-04\n",
      "    4.38165221e-04   1.39981216e-03]\n",
      " [ -7.23252880e-03  -1.10674855e-04   8.47296500e-04   1.05924755e-03\n",
      "   -3.73774101e-05   4.79983933e-03  -5.91317236e-04   4.38165222e-04\n",
      "    9.18467932e-03  -3.11969126e-02]\n",
      " [  2.87903073e-02   3.18922973e-03  -5.11070462e-02   5.71017491e-03\n",
      "    7.06930216e-04  -5.73830693e-02  -4.03001128e-03   1.39981215e-03\n",
      "   -3.11969126e-02   5.31723902e+00]]\n"
     ]
    }
   ],
   "source": [
    "raw_df2=pd.read_csv('climate_change_2.csv')\n",
    "X2=raw_df.iloc[:,:-1]\n",
    "print(np.linalg.inv(np.matmul(X2.T,X2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So for __'climate_change_2.csv'__, np.matmul(X.T,X) is also convertible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "## 1. ![](E:/formula.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](/formula.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -1.27770014e+02   4.60145297e-04  -4.15483380e-03   6.80132910e-02\n",
      "   2.52584724e-03   9.89714878e-05  -9.22491933e-03  -6.72556063e-03\n",
      "   3.90701342e-03   9.42050182e-02  -1.86768294e+00]\n"
     ]
    }
   ],
   "source": [
    "def closed_form_2(X_train,y_train,lamda):\n",
    "    X=X_train.copy()\n",
    "    X.insert(0,'intercept',np.ones(len(X)))\n",
    "    \n",
    "    I=np.identity(X.shape[1])\n",
    "#     print(I)\n",
    "    I[0,0]=0\n",
    "    return np.dot(np.matmul(np.linalg.inv(np.matmul(X.T,X)-lamda*I),X.T),y_train)\n",
    "\n",
    "lamda=0.03\n",
    "theta2=closed_form_2(X_train,y_train,lamda)\n",
    "print(theta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_Square of training set: 0.752982518478\n",
      "R_Square of test set: 0.228340101963\n"
     ]
    }
   ],
   "source": [
    "y_hat=theta2[0]+np.dot(X_train,theta2[1:])\n",
    "y_mean=np.mean(y_train)\n",
    "# print(y_average)\n",
    "print(\"R_Square of training set:\",end=\" \")\n",
    "print(1-np.dot(y_hat-y_train,y_hat-y_train)/np.dot(y_train-y_mean,y_train-y_mean))\n",
    "# print(np.array(y_train))\n",
    "\n",
    "y_test_hat=theta2[0]+np.dot(X_test,theta2[1:])\n",
    "y_test_mean=np.mean(y_test)\n",
    "# print(y_average)\n",
    "print(\"R_Square of test set:\",end=\" \")\n",
    "print(1-np.dot(y_test_hat-y_test,y_test_hat-y_test)/np.dot(y_test-y_test_mean,y_test-y_test_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compared to the result in Problem 1, the R_sqaure of test test is higher. So L2 regularization is more robust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lamda= 0.001\n",
      "R_Square of training set: 0.754940791135, R_Square of test set: 0.192213226284\n",
      "lamda= 0.01\n",
      "R_Square of training set: 0.754774881237, R_Square of test set: 0.204631387577\n",
      "lamda= 0.1\n",
      "R_Square of training set: 0.670660653475, R_Square of test set: -0.0686715985884\n",
      "lamda= 1\n",
      "R_Square of training set: 0.690347891056, R_Square of test set: -0.585899465573\n",
      "lamda= 10\n",
      "R_Square of training set: -6.65768988412, R_Square of test set: -30.2330963986\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "lamda_list=[0.001,0.01,0.1,1,10]\n",
    "for lamda in lamda_list:\n",
    "    theta2=closed_form_2(X_train,y_train,lamda)\n",
    "#     print(theta2)\n",
    "    y_hat=theta2[0]+np.dot(X_train,theta2[1:])\n",
    "    y_mean=np.mean(y_train)\n",
    "    # print(y_average)\n",
    "    print(\"lamda=\",end=\" \")\n",
    "    print(lamda)\n",
    "    print(\"R_Square of training set:\",end=\" \")\n",
    "    print(1-np.dot(y_train-y_hat,y_train-y_hat)/np.dot(y_train-y_mean,y_train-y_mean),end=\", \")\n",
    "#     print(pd.DataFrame([y_hat,y_train]))\n",
    "    \n",
    "    y_test_hat=theta2[0]+np.dot(X_test,theta2[1:])\n",
    "    y_test_mean=np.mean(y_test)\n",
    "    # print(y_average)\n",
    "    print(\"R_Square of test set:\",end=\" \")\n",
    "    print(1-np.dot(y_test_hat-y_test,y_test_hat-y_test)/np.dot(y_test-y_test_mean,y_test-y_test_mean))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3\n",
    "## 1.\n",
    "We can use PCA to reduce dimension. Principal Component Analysis (PCA) is the most widely used data dimension reduction algorithm.\n",
    "Specific algorithm work flow:  \n",
    "1)      Set M n-dimensional data:  \n",
    "2)      Form the raw data into N row M column matrix X in columns  \n",
    "3)      We're going to zero mean every row of X, so we're going to subtract the mean of every row  \n",
    "4)      Find the covariance matrix C for X  \n",
    "5)      Find the eigenvalues of the covariance matrix C and the corresponding eigenvectors. The eigenvalues of C are the variances of each dimension of Y and the diagonal elements of D.  \n",
    "6)      The eigenvectors are arranged into matrices from top to bottom according to the corresponding eigenvalues. According to the actual business scenario, the first R rows are taken to form the matrix P  \n",
    "7)      Y=PX is the target matrix after R dimension is reduced  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lamda= 0.001\n",
      "R_Square of training set: 0.750805899358, R_Square of test set: -0.0289549772534\n",
      "lamda= 0.01\n",
      "R_Square of training set: 0.750732356691, R_Square of test set: -0.0324290719715\n",
      "lamda= 0.1\n",
      "R_Square of training set: 0.750017309963, R_Square of test set: -0.0651262903629\n",
      "lamda= 1\n",
      "R_Square of training set: 0.744364431705, R_Square of test set: -0.256411387255\n",
      "lamda= 10\n",
      "R_Square of training set: 0.692707642976, R_Square of test set: -0.424000104237\n"
     ]
    }
   ],
   "source": [
    "def normalization(X_train):\n",
    "    mean=np.mean(X_train)\n",
    "    mx=np.max(X_train)\n",
    "    mn=np.min(X_train)\n",
    "    X=(X_train-mean)/(mx-mn)\n",
    "    return X,mean,mx,mn\n",
    "\n",
    "X_train_norm,mean_train,max_train,min_train=normalization(X_train)\n",
    "def gradient_Decent(X_train,y_train,lamda,alpha):\n",
    "    # 随便初始一个theta\n",
    "#     theta=closed_form_1(X_train,y_train)\n",
    "    \n",
    "    \n",
    "    iters=10000\n",
    "    X=X_train.copy()\n",
    "    X.insert(0,'intercept',np.ones(len(X)))\n",
    "    theta=np.zeros(X.shape[1])\n",
    "    \n",
    "    delta=np.matmul(X.T,np.matmul(X,theta)-y_train)\n",
    "    \n",
    "    for i in range(iters):\n",
    "        delta=np.matmul(X.T,np.matmul(X,theta)-y_train)\n",
    "        theta[0]=theta[0]-(alpha/X.shape[0])*(delta[0])\n",
    "        theta[1:]=theta[1:]-(alpha/X.shape[0])*(delta[1:]+lamda*theta[1:])\n",
    "        \n",
    "    return theta\n",
    "\n",
    "for lamda in lamda_list:\n",
    "    \n",
    "\n",
    "    theta2=gradient_decent(X_train_norm,y_train,lamda,0.1)\n",
    "#     print(theta2)\n",
    "    \n",
    "    y_hat=theta2[0]+np.matmul(X_train_norm,theta2[1:])\n",
    "    y_mean=np.mean(y_train)\n",
    "    print(\"lamda=\",end=\" \")\n",
    "    print(lamda)\n",
    "    print(\"R_Square of training set:\",end=\" \")\n",
    "    print(1-np.matmul(y_hat-y_train,y_hat-y_train)/np.matmul(y_train-y_mean,y_train-y_mean),end=\", \")\n",
    "#     print(pd.DataFrame([y_hat,y_train]))\n",
    "    \n",
    "    y_test_hat=theta2[0]+np.matmul((X_test-mean_train)/(max_train-min_train),theta2[1:])\n",
    "    y_test_mean=np.mean(y_test)\n",
    "    print(\"R_Square of test set:\",end=\" \")\n",
    "    print(1-np.matmul(y_test_hat-y_test,y_test_hat-y_test)/np.matmul(y_test-y_test_mean,y_test-y_test_mean))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
